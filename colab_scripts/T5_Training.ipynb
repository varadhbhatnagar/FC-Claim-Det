{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBU8xhWnBzKb",
    "outputId": "576ddf9b-01fa-47a5-a51b-fe9a8e35e018"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.2.1-py3-none-any.whl (342 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |█                               | 10 kB 28.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 20 kB 34.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 30 kB 13.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 40 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 51 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 61 kB 9.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 71 kB 9.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 81 kB 8.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 92 kB 9.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 102 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 112 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 122 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 133 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 143 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 153 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 163 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 174 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 184 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 194 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 204 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 215 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 225 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 235 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 245 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 256 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 266 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 276 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 286 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 296 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 307 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 317 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 327 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 337 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 342 kB 8.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 68.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 56.3 MB/s \n",
      "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
      "\u001b[K     |████████████████████████████████| 136 kB 77.2 MB/s \n",
      "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 3.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 58.2 MB/s \n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
      "\u001b[K     |████████████████████████████████| 271 kB 70.7 MB/s \n",
      "\u001b[?25hCollecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[K     |████████████████████████████████| 144 kB 59.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 3.2 MB/s \n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, huggingface-hub, datasets\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.24.3\n",
      "    Uninstalling urllib3-1.24.3:\n",
      "      Successfully uninstalled urllib3-1.24.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.2.1 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.6.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.19.0-py3-none-any.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 8.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 56.0 MB/s \n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 72.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Installing collected packages: pyyaml, tokenizers, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SGIa_TAB5KA"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments, Seq2SeqTrainingArguments, Seq2SeqTrainer, T5ForConditionalGeneration, T5TokenizerFast\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import random\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPs5MWBB9jF-"
   },
   "outputs": [],
   "source": [
    "def retrieval_eval(query, gold_urls):\n",
    "  retrieval_1 = 0\n",
    "  retrieval_5 = 0\n",
    "  retrieval_10 = 0\n",
    "  retrieval_20 = 0\n",
    "  retrieval_50 = 0\n",
    "  mrr = 0\n",
    "  reciprocal_ranks = []\n",
    "\n",
    "  for i in range(0,len(query)):\n",
    "    added = False\n",
    "    cand = query[i]\n",
    "    gold_url = urlparse(gold_urls[i])\n",
    "    gold_url_netloc = gold_url[1]\n",
    "    gold_url_path = gold_url[2]\n",
    "    if gold_url_path[-1]=='/':\n",
    "      gold_url_path = gold_url_path[:-1]\n",
    "    \n",
    "    params = {\n",
    "            'query': cand.strip(),\n",
    "            'pageSize': '50',\n",
    "            'key': '<your_key>',\n",
    "        }\n",
    "    try:\n",
    "        response_dict = requests.get(\n",
    "            'https://factchecktools.googleapis.com/v1alpha1/claims:search',\n",
    "            params=params).json()\n",
    "    except Exception as e:\n",
    "      response_dict = {}\n",
    "      print(\"Google API is not working\")\n",
    "      break\n",
    "\n",
    "    if 'claims' in response_dict.keys():\n",
    "      for i in range(0, len(response_dict[\"claims\"])):\n",
    "        claim = response_dict['claims'][i]\n",
    "        ret_url = urlparse(claim['claimReview'][0]['url'])\n",
    "        ret_url_netloc = ret_url[1]\n",
    "        ret_url_path = ret_url[2]\n",
    "        if ret_url_path[-1]=='/':\n",
    "          ret_url_path = ret_url_path[:-1]\n",
    "\n",
    "        if(ret_url_netloc == gold_url_netloc and ret_url_path == gold_url_path):\n",
    "          added = True\n",
    "          reciprocal_ranks.append(1/(i+1))\n",
    "          if i<1:\n",
    "            retrieval_1 +=1\n",
    "\n",
    "          if i<5:\n",
    "            retrieval_5 +=1\n",
    "\n",
    "          if i<10:\n",
    "            retrieval_10 +=1  \n",
    "\n",
    "          if i<20:\n",
    "            retrieval_20 +=1\n",
    "\n",
    "          retrieval_50 +=1\n",
    "          break\n",
    "    \n",
    "    if added == False:\n",
    "      reciprocal_ranks.append(0)\n",
    "\n",
    "  if len(reciprocal_ranks) >0:\n",
    "    mrr = sum(reciprocal_ranks)/len(reciprocal_ranks)\n",
    "\n",
    "  return { \n",
    "      'retrieval_1': (retrieval_1*100)/len(query),\n",
    "      'retrieval_5': (retrieval_5*100)/len(query),\n",
    "      'retrieval_10': (retrieval_10*100)/len(query),\n",
    "      'retrieval_20': (retrieval_20*100)/len(query),\n",
    "      'retrieval_50': (retrieval_50*100)/len(query),\n",
    "      'mrr': mrr,\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPm8CpBeqQii"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZ4tJX5-9wpD"
   },
   "outputs": [],
   "source": [
    "filenames = ['summ_data_eng_preprocessed_hashtag_removed.csv', 'summ_data_eng_preprocessed_mention_removed.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfsoNk4LK46J"
   },
   "outputs": [],
   "source": [
    "def t5_summarizer(df,model, min_len, max_len):\n",
    "  t5_summaries = []\n",
    "  for row in df:\n",
    "    preprocess_text = row.strip().replace(\"\\n\",\"\")\n",
    "    tokenized_text = tokenizer.encode(preprocess_text, return_tensors=\"pt\").to(device)\n",
    "    summary_ids = model.generate(tokenized_text,\n",
    "                                      num_beams=6,\n",
    "                                      no_repeat_ngram_size=2,\n",
    "                                      min_length=min_len,\n",
    "                                      max_length=max_len,\n",
    "                                      early_stopping=True)\n",
    "\n",
    "    output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    t5_summaries.append(output)\n",
    "    del tokenized_text\n",
    "  return t5_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "aa0df7153d834c27bcbb5b4a1fe34f50",
      "3702873c261c492982dab3c3af34b1b7",
      "f2464f44637142cc8feca0e1fbfb6d4a",
      "163e247e54b749fcb354bdbe03901549",
      "ef8d68823ff54991941a941dc744833c",
      "b4d01fa797d747b691ac254d915330d8",
      "e05d808bccc2438e88b253c0d36a13d5",
      "584bc73c87324445894287c72443eed5",
      "4493631c102e46b9a228ebe17e850036",
      "1f7cec68cbde4192af1288573db6e39c",
      "6ba2c79bfadc4a5bbc39bd8eaeb20563",
      "d71e7150e8b8456ea18cd67442ffb79d",
      "4a08eb616b2d422bb46a02fa8932a44a",
      "f1bc4f1a88ce47269449a5fd1a06fbf3",
      "4601d9ff59494dc697501ee266140970",
      "11efa8320e5f4050bf914a34b0117a3f",
      "1438b5d222d1434583582cbdf9946947",
      "03e2b8650a6048d495ac57e8cf11584c",
      "c9409ffc7073468cbbe093d548c2f236",
      "5d9e92eb921c495a95735fcbe7684bc6",
      "9cf162a917d3464f8d394bb97cb03ca9",
      "f88f4f47da07403280d6f19c30c91cc7"
     ]
    },
    "id": "txolpXd0DltH",
    "outputId": "3854cb84-b273-472d-e6e5-2504c0b68066"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5_fast.py:161: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0df7153d834c27bcbb5b4a1fe34f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71e7150e8b8456ea18cd67442ffb79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: __index_level_0__, target, source. If __index_level_0__, target, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 453\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 912\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py:131: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  batch[k] = torch.tensor([f[k] for f in features])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='912' max='912' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [912/912 13:04, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.862600</td>\n",
       "      <td>0.760083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.789100</td>\n",
       "      <td>0.645172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.652100</td>\n",
       "      <td>0.594348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.522100</td>\n",
       "      <td>0.565447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.474000</td>\n",
       "      <td>0.550843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.398200</td>\n",
       "      <td>0.536752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.373900</td>\n",
       "      <td>0.533535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.334100</td>\n",
       "      <td>0.529920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.322000</td>\n",
       "      <td>0.529947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: __index_level_0__, target, source. If __index_level_0__, target, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 114\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to 0/output/1/checkpoint-100\n",
      "Configuration saved in 0/output/1/checkpoint-100/config.json\n",
      "Model weights saved in 0/output/1/checkpoint-100/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: __index_level_0__, target, source. If __index_level_0__, target, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 114\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to 0/output/1/checkpoint-200\n",
      "Configuration saved in 0/output/1/checkpoint-200/config.json\n",
      "Model weights saved in 0/output/1/checkpoint-200/pytorch_model.bin\n",
      "Deleting older checkpoint [0/output/1/checkpoint-100] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: __index_level_0__, target, source. If __index_level_0__, target, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 114\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to 0/output/1/checkpoint-300\n",
      "Configuration saved in 0/output/1/checkpoint-300/config.json\n",
      "Model weights saved in 0/output/1/checkpoint-300/pytorch_model.bin\n",
      "Deleting older checkpoint [0/output/1/checkpoint-200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: __index_level_0__, target, source. If __index_level_0__, target, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 114\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to 0/output/1/checkpoint-400\n",
      "Configuration saved in 0/output/1/checkpoint-400/config.json\n",
      "Model weights saved in 0/output/1/checkpoint-400/pytorch_model.bin\n",
      "Deleting older checkpoint [0/output/1/checkpoint-300] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: __index_level_0__, target, source. If __index_level_0__, target, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 114\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to 0/output/1/checkpoint-500\n",
      "Configuration saved in 0/output/1/checkpoint-500/config.json\n",
      "Model weights saved in 0/output/1/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [0/output/1/checkpoint-400] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: __index_level_0__, target, source. If __index_level_0__, target, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 114\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to 0/output/1/checkpoint-600\n",
      "Configuration saved in 0/output/1/checkpoint-600/config.json\n",
      "Model weights saved in 0/output/1/checkpoint-600/pytorch_model.bin\n",
      "Deleting older checkpoint [0/output/1/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: __index_level_0__, target, source. If __index_level_0__, target, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 114\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to 0/output/1/checkpoint-700\n",
      "Configuration saved in 0/output/1/checkpoint-700/config.json\n",
      "Model weights saved in 0/output/1/checkpoint-700/pytorch_model.bin\n",
      "Deleting older checkpoint [0/output/1/checkpoint-600] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: __index_level_0__, target, source. If __index_level_0__, target, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 114\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to 0/output/1/checkpoint-800\n",
      "Configuration saved in 0/output/1/checkpoint-800/config.json\n",
      "Model weights saved in 0/output/1/checkpoint-800/pytorch_model.bin\n",
      "Deleting older checkpoint [0/output/1/checkpoint-700] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: __index_level_0__, target, source. If __index_level_0__, target, source are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 114\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to 0/output/1/checkpoint-900\n",
      "Configuration saved in 0/output/1/checkpoint-900/config.json\n",
      "Model weights saved in 0/output/1/checkpoint-900/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from 0/output/1/checkpoint-800 (score: 0.5299199223518372).\n",
      "Saving model checkpoint to 0/model/1/\n",
      "Configuration saved in 0/model/1/config.json\n",
      "Model weights saved in 0/model/1/pytorch_model.bin\n",
      "loading configuration file 0/model/1/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file 0/model/1/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at 0/model/1/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 1):\n",
    "  file = filenames[i]\n",
    "  summ_data = pd.read_csv(file, header=0, index_col=0)\n",
    "  index = list(pd.read_csv('index.csv', header=0, index_col=0)['index'])\n",
    "  t5_summ_data = summ_data[['tweet', 'claim_reviewed']].copy()\n",
    "  t5_summ_data.columns = ['source', 'target']\n",
    "  t5_summ_data = t5_summ_data.reindex(index)\n",
    "\n",
    "  kf = KFold(n_splits=5)\n",
    "  ct = 0\n",
    "  hf_model_name = 't5-large'\n",
    "  # %load_ext tensorboard\n",
    "  # %tensorboard --logdir output/\n",
    "\n",
    "  for train_index, val_index in kf.split(t5_summ_data):\n",
    "    if ct!=1:\n",
    "      ct = ct+1\n",
    "      continue\n",
    "    tokenizer = T5TokenizerFast.from_pretrained(hf_model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(hf_model_name)\n",
    "\n",
    "    def tokenize(batch):\n",
    "      tokenized_input = tokenizer(batch['source'], padding='longest')\n",
    "      tokenized_label = tokenizer(batch['target'], padding='longest')\n",
    "\n",
    "      tokenized_input['labels'] = tokenized_label['input_ids']\n",
    "\n",
    "      return tokenized_input\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(t5_summ_data.iloc[train_index])\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True, batch_size = len(train_dataset))\n",
    "    val_dataset = Dataset.from_pandas(t5_summ_data.iloc[val_index])\n",
    "    val_dataset = val_dataset.map(tokenize, batched=True, batch_size = len(val_dataset))\n",
    "    \n",
    "    \n",
    "    train_dataset.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    val_dataset.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    model_dir = str(i)+'/model/'+str(ct)+\"/\"\n",
    "    output_dir = str(i)+'/output/'+str(ct)+\"/\"\n",
    "    now = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    logging_dir = output_dir+\"runs/\"+now+\"/\"\n",
    "    \n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=8,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        eval_accumulation_steps=1, # Number of eval steps to keep in GPU (the higher, the mor vRAM used)\n",
    "        prediction_loss_only=True, # If I need co compute only loss and not other metrics, setting this to true will use less RAM\n",
    "        learning_rate=0.00005,\n",
    "        evaluation_strategy='steps', # Run evaluation every eval_steps\n",
    "        save_steps=100, # How often to save a checkpoint\n",
    "        save_total_limit=1, # Number of maximum checkpoints to save\n",
    "        remove_unused_columns=True, # Removes useless columns from the dataset\n",
    "        run_name='train', # Wandb run name\n",
    "        logging_steps=100, # How often to log loss to wandb\n",
    "        eval_steps=100, # How often to run evaluation on the val_set\n",
    "        logging_first_step=False, # Whether to log also the very first training step to wandb\n",
    "        load_best_model_at_end=True, # Whether to load the best model found at each evaluation.\n",
    "        metric_for_best_model=\"loss\", # Use loss to evaluate best model.\n",
    "        greater_is_better=False, # Best model is the one with the lowest loss, not highest.\n",
    "        generation_max_length = 15,\n",
    "        generation_num_beams = 6,\n",
    "        predict_with_generate=True,\n",
    "        report_to=\"tensorboard\",\n",
    "        logging_dir=logging_dir\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(model_dir)\n",
    "\n",
    "\n",
    "\n",
    "    cur_model = T5ForConditionalGeneration.from_pretrained(model_dir).to(device)\n",
    "\n",
    "    decoded_sources = val_dataset['source']\n",
    "\n",
    "    decoded_preds = t5_summarizer(val_dataset['source'],cur_model, 5, 15)\n",
    "    decoded_labels = val_dataset['target']\n",
    "\n",
    "    output = pd.DataFrame({'ID': val_dataset['__index_level_0__'], 'Source Text': decoded_sources, 'Target Text': decoded_labels, 'Generated Text': decoded_preds})\n",
    "    output.to_csv(output_dir +\"/predictions.xlsx\")\n",
    "\n",
    "    del cur_model\n",
    "    del decoded_preds\n",
    "    del model\n",
    "    del tokenizer\n",
    "\n",
    "    ct = ct +1\n",
    "\n",
    "  output_dir = str(i)+'/output/'\n",
    "  prediction_file_paths = ['0/predictions.xlsx', '1/predictions.xlsx', '2/predictions.xlsx', '3/predictions.xlsx', '4/predictions.xlsx']\n",
    "  for file in prediction_file_paths:\n",
    "    preds = pd.read_csv(output_dir+file, header=0, index_col=0)\n",
    "    gold_urls = []\n",
    "    for index, row in preds.iterrows():\n",
    "      gold_urls.append(summ_data.loc[row['ID']]['evidence_url'])\n",
    "\n",
    "    print(\"{} T5 FT Retrieval = {}\".format(file, retrieval_eval(preds['Generated Text'], gold_urls)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06KXgEPDP3vH"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JfE8IqaAYWB"
   },
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtkH7PAuBVrT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "T5_Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03e2b8650a6048d495ac57e8cf11584c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "11efa8320e5f4050bf914a34b0117a3f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1438b5d222d1434583582cbdf9946947": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "163e247e54b749fcb354bdbe03901549": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f7cec68cbde4192af1288573db6e39c",
      "placeholder": "​",
      "style": "IPY_MODEL_6ba2c79bfadc4a5bbc39bd8eaeb20563",
      "value": " 1/1 [00:00&lt;00:00,  7.30ba/s]"
     }
    },
    "1f7cec68cbde4192af1288573db6e39c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3702873c261c492982dab3c3af34b1b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4d01fa797d747b691ac254d915330d8",
      "placeholder": "​",
      "style": "IPY_MODEL_e05d808bccc2438e88b253c0d36a13d5",
      "value": "100%"
     }
    },
    "4493631c102e46b9a228ebe17e850036": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4601d9ff59494dc697501ee266140970": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9cf162a917d3464f8d394bb97cb03ca9",
      "placeholder": "​",
      "style": "IPY_MODEL_f88f4f47da07403280d6f19c30c91cc7",
      "value": " 1/1 [00:00&lt;00:00, 14.47ba/s]"
     }
    },
    "4a08eb616b2d422bb46a02fa8932a44a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1438b5d222d1434583582cbdf9946947",
      "placeholder": "​",
      "style": "IPY_MODEL_03e2b8650a6048d495ac57e8cf11584c",
      "value": "100%"
     }
    },
    "584bc73c87324445894287c72443eed5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d9e92eb921c495a95735fcbe7684bc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6ba2c79bfadc4a5bbc39bd8eaeb20563": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9cf162a917d3464f8d394bb97cb03ca9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa0df7153d834c27bcbb5b4a1fe34f50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3702873c261c492982dab3c3af34b1b7",
       "IPY_MODEL_f2464f44637142cc8feca0e1fbfb6d4a",
       "IPY_MODEL_163e247e54b749fcb354bdbe03901549"
      ],
      "layout": "IPY_MODEL_ef8d68823ff54991941a941dc744833c"
     }
    },
    "b4d01fa797d747b691ac254d915330d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9409ffc7073468cbbe093d548c2f236": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d71e7150e8b8456ea18cd67442ffb79d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4a08eb616b2d422bb46a02fa8932a44a",
       "IPY_MODEL_f1bc4f1a88ce47269449a5fd1a06fbf3",
       "IPY_MODEL_4601d9ff59494dc697501ee266140970"
      ],
      "layout": "IPY_MODEL_11efa8320e5f4050bf914a34b0117a3f"
     }
    },
    "e05d808bccc2438e88b253c0d36a13d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ef8d68823ff54991941a941dc744833c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1bc4f1a88ce47269449a5fd1a06fbf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9409ffc7073468cbbe093d548c2f236",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5d9e92eb921c495a95735fcbe7684bc6",
      "value": 1
     }
    },
    "f2464f44637142cc8feca0e1fbfb6d4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_584bc73c87324445894287c72443eed5",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4493631c102e46b9a228ebe17e850036",
      "value": 1
     }
    },
    "f88f4f47da07403280d6f19c30c91cc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
